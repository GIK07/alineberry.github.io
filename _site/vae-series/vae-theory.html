<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.3 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Variational Autoencoder Theory - Adam Lineberry</title>
<meta name="description" content="Transforming general theory into VAE-specific theory">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Adam Lineberry">
<meta property="og:title" content="Variational Autoencoder Theory">
<meta property="og:url" content="http://localhost:4000/vae-series/vae-theory">


  <meta property="og:description" content="Transforming general theory into VAE-specific theory">







  <meta property="article:published_time" content="2019-07-07T00:00:00-06:00">





  

  


<link rel="canonical" href="http://localhost:4000/vae-series/vae-theory">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Adam Lineberry",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Adam Lineberry Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">Adam Lineberry</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/machine-learning/" >Machine Learning Blog</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/images/headshot2.jpeg" alt="Adam Lineberry" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Adam Lineberry</h3>
    
    
      <p class="author__bio" itemprop="description">
        Data Scientist
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Denver, CO</span>
        </li>
      

      
        
          
            <li><a href="https://www.linkedin.com/in/adam-lineberry/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://github.com/acetherace" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
        
          
        
          
            <li><a href="https://twitter.com/adam_lineberry_" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
          
        
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Variational Autoencoder Theory">
    <meta itemprop="description" content="Transforming general theory into VAE-specific theory">
    <meta itemprop="datePublished" content="July 07, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Variational Autoencoder Theory
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p class="notice--info">This is the third post in my series: <a href="http://localhost:4000/vae-series/">From KL Divergence to Variational Autoencoder in PyTorch</a>. The previous post in the series is <a href="http://localhost:4000/vae-series/variational-inference">Latent Variable Models, Expectation Maximization, and Variational Inference</a> and the next post is <a href="http://localhost:4000/vae-series/vae-code-experiments">Variational Autoencoder Code and Experiments</a>.</p>

<hr />

<p>The Variational Autoencoder has taken the machine learning community by storm since Kingma and Welling’s seminal paper was released in 2013<sup>1</sup>. It was one of the first model architectures in the mainstream to establish a strong connection between deep learning and Bayesian statistics. Quite frankly, it’s also just really cool. A VAE trained on image data results in the ability to create spectacular visualizations of the latent factors it learns and the realistic images it can generate. In the world of data science it’s an excellent bridge between statistics and computer science. It’s interesting to think about and tinker with, and it makes a great sandbox to learn and build intuition about deep learning and statistics.</p>

<figure class="half" style="display:flex">
  <img src="http://localhost:4000/images/vae/datagen_final.png" height="100" />
  <img src="http://localhost:4000/images/vae/frey_face.png" height="100" />
  <figcaption>(left) Synthesized digits from MNIST sampled from a grid on the learned latent manifold. Notice the smooth transitions between digits. (right) Synthesized faces sampled from a grid on the manifold of a VAE trained on the Frey Face dataset<sup>1</sup>. Notice that the VAE has learned interpretable latent factors: left-to-right adjusts head orientation, top-to-bottom adjusts level of frowning or smiling. </figcaption>
</figure>

<p>It isn’t just a playground though; there are extremely valuable applications for the VAE on real world problems. It can be used for representation learning/feature engineering/dimensionality reduction to improve performance on downstream tasks such as classification models or recommender systems. You can also leverage its probabilistic nature to perform anomaly detection. Its data generation capability also lends itself to assist in the training of reinforcement learning systems.</p>

<p>The VAE seems very similar to other autoencoders. At a high level, an autoencoder is a deep neural network that is trained to reconstruct its own input. There are many variations of this fundamental idea that accomplish different end tasks, such as the vanilla autoencoder, the denoising autoencoder, and the sparse autoencoder. But the VAE stands apart from the rest in that it is a fully probabilistic model.</p>

<p>In this post we are going to introduce the theory of the VAE by building on concepts introduced in the previous post, such as variational inference and maximizing the Evidence Lower Bound (ELBO).</p>

<div class="notice">
<p><strong>Table of contents:</strong></p>
<ol>
  <li>Derivation of the VAE objective function</li>
  <li>Intuition behind the VAE objective function</li>
  <li>Model architecture</li>
  <li>Optimization</li>
  <li>Practical uses of the VAE</li>
</ol>
</div>

<h2 id="derivation-of-the-vae-objective-function">Derivation of the VAE objective function</h2>

<p>As discussed in my <a href="http://localhost:4000/vae-series/variational-inference">post on variational inference</a>, the intractable data likelihood which we would like to maximize can be decomposed into the following expression:</p>

<script type="math/tex; mode=display">\log p_{\theta}(x) = KL[q_{\phi}(z) \lVert p_{\theta}(z \lvert x)] + \underset{z}{\sum} q_{\phi}(z) \log \frac{p_{\theta}(x,z)}{q_{\phi}(z)}</script>

<p>The focus of variational inference methods, including the VAE, is to maximize the second term in this expression, commonly known as the ELBO or variational lower bound:</p>

<script type="math/tex; mode=display">\mathcal{L} = \underset{z}{\sum} q_{\phi}(z) \log \frac{p_{\theta}(x,z)}{q_{\phi}(z)}</script>

<p>In order to set the stage for the VAE, let’s rearrange <script type="math/tex">\mathcal{L}</script> slightly by first writing it as an expectation, substituting Bayes’ Rule, splitting up the logarithm, and recognizing a KL divergence term:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathcal{L} &= \underset{z}{\sum} q_{\phi}(z) \log \frac{p_{\theta}(x,z)}{q_{\phi}(z)} \\
&= \mathbb{E_{q_{\phi}(z)}} \log \frac{p_{\theta}(x,z)}{q_{\phi}(z)} \\
&= \mathbb{E_{q_{\phi}(z)}} \log \frac{p_{\theta}(x \lvert z)p_{\theta}(z)}{q_{\phi}(z)} \\
&= \mathbb{E_{q_{\phi}(z)}} \log p_{\theta}(x \lvert z) +  \mathbb{E_{q_{\phi}(z)}} \log \frac{p_{\theta}(z)}{q_{\phi}(z)} \\
&= \mathbb{E_{q_{\phi}(z)}} \log p_{\theta}(x \lvert z) -
KL[q_{\phi}(z) \lVert p_{\theta}(z)]
\end{align} %]]></script>

<p>Since <script type="math/tex">q</script> is intended to approximate the posterior <script type="math/tex">p_{\theta}(z \lvert x)</script> we will choose <script type="math/tex">q</script> to be conditional on <script type="math/tex">x</script>: <script type="math/tex">q_{\phi}(z) = q_{\phi}(z \lvert x)</script>. Now we’re ready to write down the objective function for the VAE:</p>

<script type="math/tex; mode=display">\mathcal{L} = \mathbb{E_{q_{\phi}(z \lvert x)}} \log p_{\theta}(x \lvert z) -
KL[q_{\phi}(z \lvert x) \lVert p_{\theta}(z)] \tag{1}</script>

<h2 id="intuition-behind-the-vae-objective-function">Intuition behind the VAE objective function</h2>

<p>It’s easy to get get lost in the weeds here, so let’s zoom back out to the big picture for a moment: we want to learn a latent variable model of our data that maximizes the likelihood of the observed data <script type="math/tex">x</script>. We have already shown that it is intractable to maximize this likelihood directly, so we have turned to approximating <script type="math/tex">p_{\theta}(z \lvert x)</script> with a new distribution <script type="math/tex">q_{\phi}</script> and maximizing the ELBO instead.</p>

<p>The practical items we would like to extract from this model are the ability to map data into latent space using <script type="math/tex">q_{\phi}(z \lvert x)</script> for exploration and/or dimensionality reduction, and the ability to synthesize new data by sampling from the latent space according to <script type="math/tex">p_{\theta}(z)</script> and then generating new data from <script type="math/tex">p_{\theta}(x \lvert z)</script>.</p>

<p>Now, let’s begin unpacking the objective function by defining the prior on <script type="math/tex">z</script>. The VAE sets this prior to a factorized unit Gaussian: <script type="math/tex">p_{\theta}(z) = \mathcal{N}(0, I)</script>. It can be shown that a simple Gaussian such as this can be mapped into very complicated distributions as long as the mapping function is sufficiently complex (e.g. a neural network)<sup>2</sup>. This choice also simplifies the optimization problem as we will see shortly.</p>

<p>Next, let’s discuss the first term in the objective.</p>

<script type="math/tex; mode=display">\mathbb{E_{q_{\phi}(z \lvert x)}} \log p_{\theta}(x \lvert z)</script>

<p>We want to learn two distributions, <script type="math/tex">q</script> and <script type="math/tex">p</script>. The <script type="math/tex">q</script> we learn should be able to map data points <script type="math/tex">x_i</script> into a latent representation <script type="math/tex">z_i</script> from which <script type="math/tex">p_{\theta}(x \lvert z)</script> is able to successfully reconstruct the original data point <script type="math/tex">x_i</script>. This term is something very similar to the standard reconstruction loss (e.g., MSE) used in vanilla autoencoders. In fact, under certain conditions, it can be shown that this term simplifies to be almost identical to MSE.</p>

<p>Simultaneously, the KL term is pushing <script type="math/tex">q</script> to look like our Gaussian prior <script type="math/tex">p_{\theta}(z)</script>.</p>

<script type="math/tex; mode=display">- KL[q_{\phi}(z \lvert x) \lVert p_{\theta}(z)]</script>

<p>This term is commonly interpreted as a form of regularization. It prevents the model from memorizing the training data and forces it to learn an informative latent manifold that pairs nicely with <script type="math/tex">p_{\theta}(x \lvert z)</script>. Without it, the greedy model would learn distributions <script type="math/tex">q_{\phi}(z \lvert x)</script> with zero variance, essentially degrading to a vanilla autoencoder. By enforcing <script type="math/tex">q_{\phi}(z \lvert x)</script> to have some variance, the learned <script type="math/tex">p_{\theta}(x \lvert z)</script> must be robust against small changes in <script type="math/tex">z</script>. This results in a smooth latent space <script type="math/tex">z</script> that can be reliably sampled from to generate new, realistic data, whereas sampling from the latent space of a vanilla autoencoder will almost always return junk<sup>5</sup>.</p>

<h2 id="model-architecture">Model architecture</h2>

<p>We choose <script type="math/tex">q_{\phi}(z \lvert x)</script> to be an infinite mixture of factorized multivariate Gaussians</p>

<script type="math/tex; mode=display">q_{\phi}(z \lvert x) = \mathcal{N}(\mu_{\phi}(x), diag(\sigma^2_{\phi}(x)))</script>

<p>Where the Gaussian parameters <script type="math/tex">\mu</script> and <script type="math/tex">s^2</script> are modeled as parametric functions of <script type="math/tex">x</script>. Note that <script type="math/tex">\sigma^2</script> is a vector of the diagonal elements of the covariance matrix. This choice provides us with a flexible distribution on <script type="math/tex">z</script> which is data point-specific because of its explicit conditioning on  <script type="math/tex">x</script>.</p>

<p>The VAE models the parameters of <script type="math/tex">q</script>, <script type="math/tex">\{\mu_{\phi}(x), \sigma^2_{\phi}(x)\}</script>, with a neural network that outputs a vector of means <script type="math/tex">\mu</script> and a vector of variances <script type="math/tex">\sigma^2</script> for each data point <script type="math/tex">x_i</script>.</p>

<p>Similarly, the distribution <script type="math/tex">p_{\theta}(x \lvert z)</script> is modeled as an infinite mixture of factorized distributions, where a neural network outputs parameters of the distribution. Depending on the type of data, this distribution is typically chosen to be Gaussian or Bernoulli. When working with binary data (like in the next post) the Bernoulli is used:</p>

<script type="math/tex; mode=display">p_{\theta}(x \lvert z) = \mathcal{Bern}(h_{\theta}(z))</script>

<p>Distributions <script type="math/tex">p_{\theta}(x \lvert z)</script> and <script type="math/tex">q_{\phi}(z \lvert x)</script> are learned jointly in the same neural network:</p>

<p><img src="http://localhost:4000/images/vae/vae-architecture.png" alt="" class="align-center" /></p>
<figcaption>Illustration of the VAE model architecture<sup>3</sup></figcaption>

<p>It is clear how the VAE model architecture closely resembles that of standard autoencoders. The first half of the network which is modeling <script type="math/tex">q_{\phi}(z \lvert x)</script> is known as the <em>probabilistic encoder</em> and the second half of the network which models <script type="math/tex">p_{\theta}(x \lvert z)</script> is known as the <em>probabilistic decoder</em>. This interpretation further extends the analogy between VAEs and standard autoencoders, but it should be noted that the mechanics and motivations are actually quite different.</p>

<p>The neural network weights are updated via SGD to maximize the objective function discussed previously:</p>

<script type="math/tex; mode=display">\mathcal{L} = \mathbb{E_{q_{\phi}(z \lvert x)}} \log p_{\theta}(x \lvert z) -
KL[q_{\phi}(z \lvert x) \lVert p_{\theta}(z)]</script>

<h2 id="optimization">Optimization</h2>

<p>Let’s first describe the overall flow and inner workings of this neural network. Data points <script type="math/tex">x_i</script> are fed into the encoder which produces vectors of means and variances defining a factorized Gaussian distribution at the center of the network. A latent variable <script type="math/tex">z_i</script> is then sampled from <script type="math/tex">q_{\phi}(z_i \lvert x_i)</script> and fed into the decoder. The decoder outputs another set of parameters defining <script type="math/tex">p_{\theta}(x_i \lvert z_i)</script> (as discussed previously, these parameters could be means and variances of another Gaussian, or the parameters of a multivariate Bernoulli). During training, the likelihood of the data point <script type="math/tex">x_i</script> under <script type="math/tex">p_{\theta}(x_i \lvert z_i)</script> can then be calculated using the Bernoulli PMF or Gaussian PDF, and maximized via gradient descent.</p>

<p>In addition to maximizing the data likelihood, which corresponds to the first term in the objective function, the KL divergence between the encoder distribution <script type="math/tex">q_{\phi}(z \lvert x)</script> and the prior <script type="math/tex">p_{\theta}(z)</script> is also minimized. Thankfully, since we have chosen Gaussians for both the prior and the approximate posterior <script type="math/tex">q_{\phi}</script>, the KL divergence term has a closed form solution which we can plug into our favorite deep learning framework.</p>

<p>Performing gradient descent on the first term also presents additional complications. For one, computing the actual expectation over <script type="math/tex">q_{\phi}</script> requires an intractable integral (i.e., computing <script type="math/tex">\log p_{\theta}(x \lvert z)</script> for all possible values of <script type="math/tex">z</script>). Instead, this expectation is approximated by Monte Carlo sampling. The Monte Carlo approximation states that the expectation of a function can be approximated by the average value of the function across <script type="math/tex">N_s</script> samples from the distribution:</p>

<script type="math/tex; mode=display">\mathbb{E_{q_{\phi}(z \lvert x)}} \log p_{\theta}(x \lvert z) \approx
\frac{1}{N_s}\sum_{s=1}^{N_s} \log p_{\theta}(x \lvert z_s)</script>

<p>In the case of the VAE we approximate the expectation using the single sample from <script type="math/tex">q_{\phi}(z \lvert x)</script> that we’ve already discussed. This is an unbiased estimate that converges over the training loop.</p>

<p>Another gradient descent-related complication is the sampling step that occurs between the encoder and the decoder. Without getting into the details, directly sampling <script type="math/tex">z</script> from <script type="math/tex">q_{\phi}(z \lvert x)</script> introduces a discontinuity that backpropogation cannot backpropogate through.</p>

<p><img src="http://localhost:4000/images/vae/architecture-no-reparam.png" alt="" width="500" class="align-center" /></p>
<figcaption>
Diagram of the VAE without the reparameterization trick. Dashed arrows represent the sampling operation.
</figcaption>

<p>The neat solution to this is called the <em>reparameterization trick</em>, which moves the stochastic operation to an input layer and results in continuous linkage between the encoder and decoder allowing for backpropogation all the way through the encoder. Instead of sampling directly from the encoder <script type="math/tex">z_i \sim q_{\phi}(z_i \lvert x_i)</script>, we can represent <script type="math/tex">z_i</script> as a deterministic function of <script type="math/tex">x_i</script> and some noise <script type="math/tex">\epsilon_i</script>:</p>

<script type="math/tex; mode=display">z_i = g_{\phi}(x_i, \epsilon_i) = \mu_{\phi}(x_i) + diag(\sigma_{\phi}(x_i)) \cdot \epsilon_i \\
\epsilon_i \sim \mathcal{N}(0, I)</script>

<p>You can show that <script type="math/tex">z</script> defined in this way follows the distribution <script type="math/tex">q_{\phi}(z \lvert x)</script>.</p>

<p><img src="http://localhost:4000/images/vae/architecture-with-reparam.png" alt="" width="550" class="align-center" /></p>
<figcaption>
Diagram of the VAE with the reparameterization trick. Dashed arrows represent the sampling operation.
</figcaption>

<h2 id="practical-uses-of-vae">Practical uses of VAE</h2>

<p>Probably the most famous use of the VAE is to generate/synthesize/hallucinate new data. The synthesis procedure is very simple: draw a random sample from the prior <script type="math/tex">p_{\theta}(z)</script>, and feed that sample through the decoder <script type="math/tex">p_{\theta}(x \lvert z)</script> to produce a new <script type="math/tex">x</script>. Since the decoder outputs distribution parameters and not real data, you can take the most probable <script type="math/tex">x</script> from this distribution. When the decoder is Gaussian, this equates to simply taking the mean vector. When it’s Bernoulli, simply round the probabilities to the nearest integer <script type="math/tex">\in \{0, 1\}</script>. Note that for data generation purposes, you can effectively throw away the encoder.</p>

<p>Another practical use is representation learning. It is certainly possible that using the latent representation of your data will improve performance of downstream tasks, such as clustering or classification. After training the VAE you can transform your data by passing it through the encoder and taking the most probable latent vectors <script type="math/tex">z</script> (which equates to taking the mean vector outputted from the encoder). Data outside of the training set can also be transformed by a previously-trained VAE. Of course, performance will be best when the new data is similar to the training data, i.e., comes from the same domain or natural distribution. As an extreme example, it probably wouldn’t make much sense to transform medical image data using a VAE that was trained on MNIST.</p>

<p>Yet another use is anomaly detection. There are various ways to leverage the probabilistic nature of the VAE to determine when a new data point is very improbable and therefore anomalous. For example, you could pass the new data through the encoder and measure the KL divergence between the encoder’s distribution and the prior. A high KL divergence would indicate that the new data is dissimilar to the data the VAE saw during training.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post we introduced the VAE and showed how it is a modern extension of the same theory that motivates the classical expectation maximization algorithm. We also derived the VAE’s objective function and explained some of the intuition behind it.</p>

<p>Some of the important details regarding the neural network architecture and optimization were discussed. We saw how the probabilistic encoder and probabilistic decoder are modeled as neural networks and how the reparameterization trick is used to allow for backpropogation through the entire network.</p>

<p>To see the VAE in action, check out my <a href="http://localhost:4000/vae-series/vae-code-experiments">next post</a> which draws a strong connection between the theory presented here and actual PyTorch code and presents the results of several interesting experiments.</p>

<h2 id="resources">Resources</h2>

<p>[1] Diederik P. Kingma, Max Welling, <a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></p>

<p>[2] Carl Doersch, <a href="https://arxiv.org/abs/1606.05908">Tutorial on Variational Autoencoders</a></p>

<p>[3] Rebecca Vislay Wade, <a href="https://www.kaggle.com/rvislaywade/visualizing-mnist-using-a-variational-autoencoder">Visualizing MNIST with a Deep Variational Autoencoder</a></p>

<p>[4] Volodymyr Kuleshov, Stefano Ermon, <a href="https://ermongroup.github.io/cs228-notes/extras/vae/">The variational auto-encoder</a></p>

<p>[5] Irhum Shafkat, <a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">Intuitively Understanding Variational Autoencoders</a></p>

<p>[6] Daniil Polykovskiy, Alexander Novikov, National Research University Higher School of Economics, Coursera, <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning">Bayesian Methods for Machine Learning</a></p>

<p>[7] Martin Krasser, <a href="http://krasserm.github.io/2018/04/03/variational-inference/">From expectation maximization to stochastic variational inference</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#machine-learning" class="page__taxonomy-item" rel="tag">machine learning</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-07-07T00:00:00-06:00">July 07, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Variational+Autoencoder+Theory%20http%3A%2F%2Flocalhost%3A4000%2Fvae-series%2Fvae-theory" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fvae-series%2Fvae-theory" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fvae-series%2Fvae-theory" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/vae-series/vae-code-experiments" class="pagination--pager" title="Variational Autoencoder Code and Experiments
">Previous</a>
    
    
      <a href="/vae-series" class="pagination--pager" title="Blog Post Series: From KL Divergence to Variational Autoencoder in PyTorch
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series" rel="permalink">Blog Post Series: From KL Divergence to Variational Autoencoder in PyTorch
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Landing page for the blog post series
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series/vae-code-experiments" rel="permalink">Variational Autoencoder Code and Experiments
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  17 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Final connections between theory and lines of code
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series/variational-inference" rel="permalink">Latent Variable Models, Expectation Maximization, and Variational Inference
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Introduction to the general theory of variational inference
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series/kl-divergence" rel="permalink">A Quick Primer on KL Divergence
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Introductory discussion on KL divergence with an emphasis on building intuition from the mathematics
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="search" id="search" aria-placeholder="Enter your search term..." class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Adam Lineberry. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.1/js/all.js" integrity="sha384-g5uSoOSBd7KkhAMlnQILrecXvzst9TdC09/VM+pjDTCM+1il8RHz5fKANTFFb+gQ" crossorigin="anonymous"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  </body>
</html>
