<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.3 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>A Quick Primer on KL Divergence - Adam Lineberry</title>
<meta name="description" content="Introductory discussion on KL divergence with an emphasis on building intuition from the mathematics">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Adam Lineberry">
<meta property="og:title" content="A Quick Primer on KL Divergence">
<meta property="og:url" content="http://localhost:4000/vae-series/kl-divergence">


  <meta property="og:description" content="Introductory discussion on KL divergence with an emphasis on building intuition from the mathematics">







  <meta property="article:published_time" content="2019-07-07T00:00:00-06:00">





  

  


<link rel="canonical" href="http://localhost:4000/vae-series/kl-divergence">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Adam Lineberry",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Adam Lineberry Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">Adam Lineberry</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/machine-learning/" >Machine Learning Blog</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/images/headshot2.jpeg" alt="Adam Lineberry" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Adam Lineberry</h3>
    
    
      <p class="author__bio" itemprop="description">
        Data Scientist
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Denver, CO</span>
        </li>
      

      
        
          
            <li><a href="https://www.linkedin.com/in/adam-lineberry/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://github.com/acetherace" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
        
          
        
          
            <li><a href="https://twitter.com/adam_lineberry_" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
          
        
          
        
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="A Quick Primer on KL Divergence">
    <meta itemprop="description" content="Introductory discussion on KL divergence with an emphasis on building intuition from the mathematics">
    <meta itemprop="datePublished" content="July 07, 2019">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">A Quick Primer on KL Divergence
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p class="notice--info">This is the first post in my series: <a href="http://localhost:4000/vae-series/">From KL Divergence to Variational Autoencoder in PyTorch</a>. The next post in the series is <a href="http://localhost:4000/vae-series/variational-inference">Latent Variable Models, Expectation Maximization, and Variational Inference</a>.</p>

<hr />

<p>The Kullback-Leibler divergence, better known as <em>KL divergence</em>, is a way to measure the “distance” between two probability distributions over the same variable. In this post we will consider distributions <script type="math/tex">q</script> and <script type="math/tex">p</script> over the random variable <script type="math/tex">z</script>.</p>

<p>It’s beneficial to be able to recognize the different forms of the KL divergence equation when studying derivations or writing your own equations.</p>

<p>For discrete random variables it takes the forms:</p>

<script type="math/tex; mode=display">KL[ q \lVert p ] = \sum\limits_{z} q(z) \log\frac{q(z)}{p(z)} = -\sum\limits_{z} q(z)\log\frac{p(z)}{q(z)}</script>

<p>For continuous random variables it takes the forms:</p>

<script type="math/tex; mode=display">KL[ q \lVert p ] = \int q(z) \log \frac{q(z)}{p(z)}dz = - \int q(z) \log \frac{p(z)}{q(z)}dz</script>

<p>And in general it can be written as an expected value:</p>

<script type="math/tex; mode=display">KL[ q \lVert p ] = \mathbb{E_{q(z)}} \log \frac{q(z)}{p(z)} = - \mathbb{E_{q(z)}} \log \frac{p(z)}{q(z)}</script>

<p>To build some intuition, let’s focus on the following form:</p>

<script type="math/tex; mode=display">KL[ q \lVert p ] = \mathbb{E_{q(z)}} \log \frac{q(z)}{p(z)}</script>

<p>Notice that the term <script type="math/tex">\log q(z)/p(z)</script> is the difference between two log probabilities: <script type="math/tex">\log q(z) - \log p(z)</script>. So, the intuition stems from the fact that KL divergence is the expected difference in log probabilities over <script type="math/tex">z</script>. Although not entirely technically correct, imagine the following to help build an intuition: consider two, perhaps similar, univariate probability density functions <script type="math/tex">q(z)</script> and <script type="math/tex">p(z)</script> and imagine sliding across the domain of <script type="math/tex">z</script> and observing the difference <script type="math/tex">q(z)-p(z)</script> at every point. This is kind of how KL divergence quantifies the “distance” between two distributions.</p>

<p>Now, a couple of important properties that I won’t prove:</p>

<script type="math/tex; mode=display">KL[q||p] \neq KL[p||q]</script>

<script type="math/tex; mode=display">KL[q||p] \geq 0 \quad \forall q, p</script>

<p>The asymmetric property begs the question: should I use <script type="math/tex">KL[q\|p]</script> or <script type="math/tex">KL[p\|q]</script>? This leads to the subject of forward versus reverse KL divergence.</p>

<h2 id="forward-vs-reverse-kl-divergence">Forward vs. Reverse KL Divergence</h2>

<p>In practice, KL divergence is typically used to learn an approximate probability distribution <script type="math/tex">q</script> to estimate a theoretic but intractable distribution <script type="math/tex">p</script>. Typically <script type="math/tex">q</script> will be of simpler form than <script type="math/tex">p</script>, since <script type="math/tex">p</script>’s complexity is what drives us to approximate it in the first place. As a simple example, <script type="math/tex">p</script> could be a bimodal distribution and <script type="math/tex">q</script> a unimodal one. When thinking about forward versus backward KL, think of <script type="math/tex">p</script> as fixed and <script type="math/tex">q</script> as something fluid that we are free to mold to <script type="math/tex">p</script>.</p>

<p>Forward KL takes the form</p>

<script type="math/tex; mode=display">KL[ p || q ] = \sum\limits_{z}p(z) \log\frac{p(z)}{q(z)}</script>

<p>As you can see from this equation and the figure below, there is a penalty anywhere <script type="math/tex">p(z) > 0</script> that <script type="math/tex">q</script> is not covering. In fact, if <script type="math/tex">q(z)=0</script> in a region where <script type="math/tex">p(z)>0</script>, the KL divergence blows up because <script type="math/tex">\lim_{q(z) \to 0} \log \frac{p(z)}{q(z)} \to \infty</script>. This results in learning a <script type="math/tex">q</script> that spreads out to cover all regions where <script type="math/tex">p</script> has any density. This is known as “zero avoiding”.</p>

<p><img src="http://localhost:4000/images/vae/forward-KL.png" alt="" width="400" class="align-center" /></p>
<figcaption>Illustration of the "zero-avoiding" behavior of forward KL. Shows a reasonable distribution q with a high forward KL divergence (top), and a different distribution q with a lower forward KL divergence (bottom).</figcaption>

<p>Reverse KL takes the form</p>

<script type="math/tex; mode=display">KL[ q || p ] = \sum\limits_{z}q(z) \log\frac{q(z)}{p(z)}</script>

<p>As seen from the equation and the figure below, reverse KL has a much different behavior. Now, the KL divergence will blow up anywhere <script type="math/tex">p(z)=0</script> unless the weighting term <script type="math/tex">q(z)=0</script>. In other words, <script type="math/tex">q(z)</script> is encouraged to be zero everywhere that <script type="math/tex">p(z)</script> is zero. This is called “zero-forcing” behavior.</p>

<p>For example, if <script type="math/tex">p</script> has probability density in two disjoint regions in space, a <script type="math/tex">q</script> with limited complexity may not be able to span the zero-probability space between these regions. In this case, the learned <script type="math/tex">q</script> would only have density in one of the two dense regions of <script type="math/tex">p</script>.</p>

<p><img src="http://localhost:4000/images/vae/reverse-KL.png" alt="" width="400" class="align-center" /></p>
<figcaption>Illustration of the "zero-forcing" behavior of reverse KL. Shows a reasonable distribution q with a high reverse KL divergence (top), and a different distribution q with a lower reverse KL divergence (bottom).</figcaption>

<h2 id="conclusion">Conclusion</h2>

<p>KL divergence is roughly a measure of distance between two probability distributions. There are different forms of the KL divergence equation. You can bring a negative out front by flipping the fraction inside the logarithm. You can also write it as an expectation.</p>

<p>Numerous machine learning models and algorithms use KL divergence as part of their loss function. By exploiting the structure of the specific model at hand, the KL divergence equation can often be simplified and optimized via gradient descent.</p>

<p>KL divergence is asymmetric and it’s important to understand the differences between forward and reverse KL.</p>

<p>My <a href="http://localhost:4000/vae-series/variational-inference">next post</a> builds on KL divergence to explore latent variable models, expectation maximization, variational inference, and the ELBO.</p>

<h2 id="resources">Resources</h2>

<p>[1] Eric Jang, <a href="https://blog.evjang.com/2016/08/variational-bayes.html">A Beginner’s Guide to Variational Methods: Mean-Field Approximation</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#machine-learning" class="page__taxonomy-item" rel="tag">machine learning</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2019-07-07T00:00:00-06:00">July 07, 2019</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=A+Quick+Primer+on+KL+Divergence%20http%3A%2F%2Flocalhost%3A4000%2Fvae-series%2Fkl-divergence" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fvae-series%2Fkl-divergence" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fvae-series%2Fkl-divergence" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/how-auroc-is-calculated/" class="pagination--pager" title="Interesting Details about ROC Curve Calculations
">Previous</a>
    
    
      <a href="/vae-series/variational-inference" class="pagination--pager" title="Latent Variable Models, Expectation Maximization, and Variational Inference
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series" rel="permalink">Blog Post Series: From KL Divergence to Variational Autoencoder in PyTorch
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Landing page for the blog post series
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series/vae-theory" rel="permalink">Variational Autoencoder Theory
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  15 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Transforming general theory into VAE-specific theory
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series/vae-code-experiments" rel="permalink">Variational Autoencoder Code and Experiments
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Final connections between theory and lines of code
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vae-series/variational-inference" rel="permalink">Latent Variable Models, Expectation Maximization, and Variational Inference
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Introduction to the general theory of variational inference
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="search" id="search" aria-placeholder="Enter your search term..." class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Adam Lineberry. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.1/js/all.js" integrity="sha384-g5uSoOSBd7KkhAMlnQILrecXvzst9TdC09/VM+pjDTCM+1il8RHz5fKANTFFb+gQ" crossorigin="anonymous"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



  </body>
</html>
