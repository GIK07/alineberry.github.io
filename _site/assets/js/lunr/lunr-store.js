var store = [{
        "title": "Interesting Details about ROC Curve Calculations",
        "excerpt":"The area under the receiver operating characteristic curve (commonly known as “AUC” or “AUROC”) is a widely used metric for evaluating binary classifiers. Most data scientists are familiar with the famous curve itself, which plots the true positive rate against the false positive rate, and are familiar with integrals (i.e.,...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/how-auroc-is-calculated/",
        "teaser":null},{
        "title": "[DRAFT] A Quick Primer on KL Divergence",
        "excerpt":"The Kullback-Leibler divergence, better known as simply KL divergence is a way to measure the “distance” between two probability distributions over the same variable. In this post we will consider distributions and over the random variable . There are multiple ways to write the KL divergence equation, and it’s beneficial...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/kl-divergence",
        "teaser":null},{
        "title": "[DRAFT] Latent Variable Models, Expectation Maximization, and Variational Inference",
        "excerpt":"Latent variable models are a powerful form of unsupervised machine learning used for a variety of tasks such as clustering, dimensionality reduction, data generation, and topic modeling. The basic premise is that there is some latent and unobserved variable that causes the observed data . The graphical model (or Bayesian...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/variational-inference",
        "teaser":null},{
        "title": "[DRAFT] Variational Autoencoder Code and Experiments",
        "excerpt":"In this post we will build and train a variational autoencoder (VAE) in PyTorch, tying everything back to the theory derived in my post on VAE theory. The first half of the post provides discussion on the key points in the implementation. The second half provides the code itself along...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/vae-code-experiments",
        "teaser":null},{
        "title": "[DRAFT] Variational Autoencoder Theory",
        "excerpt":"Since the seminal paper was released in 2015 the Variational Autoencoder (VAE) has become extremely popular. It was one of the first model architectures in the mainstream to establish a strong connection between deep learning and Bayesian statistics. It’s also just really cool. You can synthesize new data with it...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/vae-theory",
        "teaser":null},{
        "title": "[DRAFT] Blog Post Series: From KL Divergence to Variational Autoencoder in PyTorch",
        "excerpt":"In this series of four posts, I attempt to build up the theory, mathematics, and intuition of variational autoencoders (VAE), starting with some basic fundamentals and then moving closer and closer to a full PyTorch implementation with each post. Without sacrificing technical rigor, I try to demystify things along the...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series",
        "teaser":null}]
