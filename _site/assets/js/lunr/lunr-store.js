var store = [{
        "title": "Interesting Details about ROC Curve Calculations",
        "excerpt":"The area under the receiver operating characteristic curve (commonly known as “AUC” or “AUROC”) is a widely used metric for evaluating binary classifiers. Most data scientists are familiar with the famous curve itself, which plots the true positive rate against the false positive rate, and are familiar with integrals (i.e.,...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/how-auroc-is-calculated/",
        "teaser":null},{
        "title": "A Quick Primer on KL Divergence",
        "excerpt":"This is the first post in my series: From KL Divergence to Variational Autoencoder in PyTorch. The next post in the series is Latent Variable Models, Expectation Maximization, and Variational Inference. The Kullback-Leibler divergence, better known as KL divergence, is a way to measure the “distance” between two probability distributions...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/kl-divergence",
        "teaser":null},{
        "title": "Latent Variable Models, Expectation Maximization, and Variational Inference",
        "excerpt":"This is the second post in my series: From KL Divergence to Variational Autoencoder in PyTorch. The previous post in the series is A Quick Primer on KL Divergence and the next post is Variational Autoencoder Theory. Latent variable models are a powerful form of [typically unsupervised] machine learning used...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/variational-inference",
        "teaser":null},{
        "title": "Variational Autoencoder Code and Experiments",
        "excerpt":"This is the fourth and final post in my series: From KL Divergence to Variational Autoencoder in PyTorch. The previous post in the series is Variational Autoencoder Theory. In this post we will build and train a variational autoencoder (VAE) in PyTorch, tying everything back to the theory derived in...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/vae-code-experiments",
        "teaser":null},{
        "title": "Variational Autoencoder Theory",
        "excerpt":"This is the third post in my series: From KL Divergence to Variational Autoencoder in PyTorch. The previous post in the series is Latent Variable Models, Expectation Maximization, and Variational Inference and the next post is Variational Autoencoder Code and Experiments. The Variational Autoencoder has taken the machine learning community...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series/vae-theory",
        "teaser":null},{
        "title": "Blog Post Series: From KL Divergence to Variational Autoencoder in PyTorch",
        "excerpt":"In this series of four posts, I attempt to build up the theory, mathematics, and intuition of variational autoencoders (VAE), starting with some basic fundamentals and then moving closer and closer to a full PyTorch implementation with each post. Illustration of the VAE model architecture The ultimate goal of the...","categories": [],
        "tags": ["machine learning"],
        "url": "http://localhost:4000/vae-series",
        "teaser":null}]
