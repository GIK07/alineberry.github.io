---
title: "Inference for Data Density in Variational Autoencoders"
date: 2019-07-28
tags: [machine learning]
excerpt: "something"
mathjax: true
author_profile: false
permalink: /vae-series/density-inference
---

## Inferring $$p_{\theta}(x)$$ from a trained VAE

Prior on $$z$$:

$$
p_{\theta}(z) = \mathcal{N}(0, I)
$$

Want to compute $$p_{\theta}(x)$$ via Importance Sampling:

$$
\begin{align}
p_{\theta}(x) &= \int p_{\theta}(x, z)dz = \int p_{\theta}(x|z)p_{\theta}(z)dz \\
&= \mathbb{E_{p_{\theta}(z)}} p_{\theta}(x|z) \\
&= \int p_{\theta}(x|z) \frac{q_{\phi}(z \lvert x)}{q_{\phi}(z \lvert x)} p_{\theta}(z) dz \\
&= \int \frac{p_{\theta}(x|z) p_{\theta}(z)}{q_{\phi}(z \lvert x)} q_{\phi}(z \lvert x) dz \\
&= \mathbb{E_{q_{\phi}(z \lvert x)}} \frac{p_{\theta}(x|z) p_{\theta}(z)}{q_{\phi}(z \lvert x)}

\end{align}
$$

The expected value is actually computed via Monte Carlo approximation:

$$
\mathbb{E_{q_{\phi}(z \lvert x)}} \frac{p_{\theta}(x|z) p_{\theta}(z)}{q_{\phi}(z \lvert x)} \approx
\frac{1}{N_s}\sum_{s=1}^{N_s} \frac{p_{\theta}(x|z_s) p_{\theta}(z_s)}{q_{\phi}(z_s \lvert x)}
$$

In order to prevent numerical underflow, work in log space and use the LogSumExp trick to compute the summation:

$$
\begin{align}
\log p_{\theta}(x) &\approx
\log \frac{1}{N_s}\sum_{s=1}^{N_s} \frac{p_{\theta}(x|z_s) p_{\theta}(z_s)}{q_{\phi}(z_s \lvert x)} \\

&= -\log(N_s) + \log \bigg( \sum_{s=1}^{N_s} \frac{p_{\theta}(x|z_s) p_{\theta}(z_s)}{q_{\phi}(z_s \lvert x)} \bigg) \\

&= -\log(N_s) + \log \bigg( \sum_{s=1}^{N_s} \exp \log \frac{p_{\theta}(x|z_s) p_{\theta}(z_s)}{q_{\phi}(z_s \lvert x)} \bigg) \\

&= -\log(N_s) + \log \bigg( \sum_{s=1}^{N_s} \exp \big[ \log p_{\theta}(x \lvert z_s) + \log p_{\theta}(z_s) - \log q_{\phi}(z_s \lvert x) \big] \bigg)
\end{align}
$$


## Design of LSTM probabilistic decoder

SQL queries represented as a vector of tokens

$$
x = [ x_1, x_2, \dots x_s ]
$$

Where each $$x_j$$ is a SQL token (e.g., "(", "UPDATE", "VACCT_DETAIL") and $$s$$ is the length of the query.

$$
\begin{align}
\log p(x \lvert z) &=
\log p(x_1|z)p(x_2|z, x_1)p(x_3 | z, x_1, x_2)\dots(x_s|z, x_1, \dots, x_{s-1}) \\

&= \log \prod_{j=1}^s p(x_j \lvert z, x_1, \dots, x_{j-1}) \\

&= \sum_{j=1}^s \log p(x_j \lvert z, x_1, \dots, x_{j-1})
\end{align}
$$

The probability $$p(x_j \lvert z, x_1, \dots, x_{j-1})$$ is modeled by the decoder LSTM where **the latent variable $$z$$ is used to initialize the hidden state at the first timestep** and a cross entropy loss is computed at each timestep in the reconstruction.

During training of the VAE, the LSTM outputs are passed through `nn.CrossEntropyLoss` which applies a `nn.LogSoftmax()` followed by `nn.NLLLoss()`. This means the LSTM outputs are *unnormalized log probabilities*. When computing $$\log p(x \lvert z)$$, the LSTM outputs are passed through `nn.LogSoftmax` to convert them to log probabilities. At this point you have a discrete log probability distribution over the vocabular at each timestep of the decoder. Next, at each timestep, extract the component corresponding to the true $$x_i$$ at that timestep. At this point, you have a single log probability at each timestep. Next, compute the summation in the last line of the above equation.

## Bag of words data representation and decoder design

Data points are vectors of word counts $$x = [x_1, x_2, \dots, x_k]$$ and $$k$$ is the size of the vocabulary.

$$
p(x \lvert z) = p(x_1 \lvert z) p(x_2 \lvert z) \dots p(x_k \lvert z)
$$

## Design of Poisson Probabilistic Decoder

$$
\lambda = [\lambda_1, \lambda_2, \dots, \lambda_k]
$$

## Design of Multinomial Probabilistic Decoder

The decoder will output $$k$$ multinomial parameters for each data point $$x$$

$$
\theta = [\theta_1, \theta_2, \dots, \theta_k]
$$

The conditional likelihood is then

$$
p_{\theta}(x \lvert z) = \prod_j (\theta_j)^{x_j}
$$

It's easier for a neural network to output the log of these parameters, so in the actual implementation the decoder network will output $$ \log \theta  $$. The conditional likelihood expressed in log space:

$$
\log p_{\theta}(x \lvert z) =
\sum_j x_j \log \theta_j
$$
